{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detect ARs from individual time steps\n",
    "\n",
    "\n",
    "This notebook detects ARs at instantaneous time steps.\n",
    "\n",
    "The detection is performed mainly on the anomalous IVT computed in the previous step (in notebook `2 compute_THR`), using these steps:\n",
    "\n",
    "1. At any time point, find all contiguous regions from the `ivt_ano` field where $ivt_ano \\ge 0\\, kg/m/s$.\n",
    "2. Compute the centroid of all such regions, using the underlying IVT values as weights.\n",
    "3. Discard all regions whose area is $\\le 50 \\times 10^4\\, km^2$, or $\\ge 1800 \\times 10^4\\,km^2$.\n",
    "4. Discard all regions whose centroid lies north of $80^{\\circ}\\, N$, or south of $20^{\\circ} N$.\n",
    "5. Discard all regions whose  [isoperimeteric quotient](https://en.wikipedia.org/wiki/Isoperimetric_inequality) ($q = \\frac{4 \\pi A}{L^2} \\ge 0.6$) This is to filter out circular features like tropical cyclones.\n",
    "6. Compute the AR axis.\n",
    "7. Discard all regions whose AR axis is $\\le 1500\\, km$.\n",
    "8. Compute the effective width as area/length, and the length/width ratio.\n",
    "9. Discard all regions whose length/width ratio is $\\le 2$.\n",
    "\n",
    "All passing systems after the above steps are regarded as ARs.\n",
    "\n",
    "There are some more details given in the parameter selection section down below.\n",
    "\n",
    "In production you will be using the `river_tracker1.py` for this step.\n",
    "\n",
    "\n",
    "## Input data\n",
    "\n",
    "* `uflux_m1-60_6_1984_crop.nc`: u-component of vertically integrated vapor fluxes, in kg/m/s.\n",
    "* `vflux_m1-60_6_1984_crop.nc`: v-component of vertically integrated vapor fluxes, in kg/m/s.\n",
    "* `ivt_m1-60_6_1984_crop-minimal-rec-ano-kernel-t16-s6.nc`: decomposition of IVT using the THR algorithm, into a reconstruction component (`ivt_rec`) and the anomaly component (`ivt_ano`), in kg/m/s.\n",
    "\n",
    "\n",
    "## Steps\n",
    "\n",
    "1. Make sure you have successfully run the previous notebook.\n",
    "2. Execute the following code blocks in sequence.\n",
    "\n",
    "\n",
    "## Results\n",
    "\n",
    "\n",
    "* `ar_records_1984-01-01_00-00-00-1984-06-30_18-00-00.csv`: a csv table listing various attributes for each detected AR appearance at all time steps.\n",
    "* `ar_s_6_1984_label-angle-flux.nc`: a netCDF data file saving these 3 variables:\n",
    "    * `label`: an integer label distinguishing all ARs detected at individual time steps.\n",
    "    * `angles`: the angle between horizontal vapor flux and the local AR axis, in degrees.\n",
    "    * `ivt_cross`: cross-sectional IVT flux, computed as the product of IVT vector and cosine of `angles`.\n",
    "* `plots/ar_1984-01-02 18:00.png` (optional): plots of IVT and detected ARs at individual time steps.\n",
    "\n",
    "All results will be saved to the same folder as this notebook file.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set global parameters\n",
    "\n",
    "`YEAR`, `TIME_START`, and `TIME_END` are used to specify the time domain to process, and provide\n",
    "some time information for output saving.\n",
    "\n",
    "In this notebook only a small time subset is specified. In production you would want to set the time points to suit your needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------Time range--------------------\n",
    "YEAR=1984\n",
    "TIME_START='%d-03-01 00:00:00' %YEAR\n",
    "TIME_END='%d-03-09 18:00:00' %YEAR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify the input and output locations. A subfolder is created using the `YEAR` defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os, sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "#-----------u-qflux----------------------\n",
    "UQ_FILE_NAME=os.path.join('.', 'uflux_m1-60_6_1984_crop.nc')\n",
    "UQ_VAR='uflux'\n",
    "\n",
    "#-----------v-qflux----------------------\n",
    "VQ_FILE_NAME=os.path.join('.', 'vflux_m1-60_6_1984_crop.nc')\n",
    "VQ_VAR='vflux'\n",
    "\n",
    "#-----------------ivt reconstruction and anomalies-----------------\n",
    "IVT_FILE_NAME=os.path.join('.', 'ivt_m1-60_6_1984_crop-minimal-rec-ano-kernel-t16-s6.nc')\n",
    "\n",
    "#------------------Output folder------------------\n",
    "OUTPUTDIR=os.path.join('.', str(YEAR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `PLOT` controls whether to plot figures of IVT with detected ARs.\n",
    "* `SINGLE_DOME` controls whether to separate local maxima that have been merged together.\n",
    "* `LAT1` and `LAT2` selects the latitude range to process.\n",
    "* `RESO` is the (approximate) horizontal resolution of input data. For the ERA-I data used in this notebook it is 0.75.\n",
    "* `SHIFT_LON` shifts the data along the x-dimension by 80 degrees so the Pacific and Atlantic oceans are centered.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PLOT=True          # create maps of found ARs or not\n",
    "SINGLE_DOME=False  # do peak partition or not\n",
    "\n",
    "LAT1=0; LAT2=90      # degree, latitude domain\n",
    "# NOTE: this has to match the domain selection in compute_thr_singlefile.py\n",
    "\n",
    "RESO=0.75             # degree, (approximate) horizontal resolution of input data.\n",
    "SHIFT_LON=80          # degree, shift left bound to longitude. Should match\n",
    "                      # that used in compute_thr_singlefile.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `PARAM_DICT` dictionary contains important parameters used in the detection process:\n",
    "\n",
    "* `thres_low`: float, a minimum IVT value in $kg/m/s$. This tells the script to look for AR candidates as regions where the anomaly IVT (`ivt_ano`) >= this value. This is the same idea as the IVT250 thresholding method. In production one should give it a small nominal value like `1`, `10` etc., or just `0`.\n",
    "* `min_area`: float, minimum area in $km^2$. Drop AR candidates smaller than this area. Region of area is defined as the summation of grid cell areas, computed using the latitude/longitude meta data of input data. This is used to filter out some miniature features.\n",
    "* `max_area`: float, maximum area in $km^2$. Filter out regions too large in size. This might happen when 2 AR-like features get merged together. You can prevent this from happening by raising the `thres_low` value, or setting `SINGLE_DOME` to true.\n",
    "* `max_isoq_hard`: float in (0,1), the maximum isoperimeteric quotient. This is to filter out circular features like tropical cyclones.\n",
    "* `max_isoq`: AR candidates with isoperimeteric quotient larger than this values is flagged as `relaxed`, meaning that they may not be a typical AR, but kind of close. This effectively serves as a simple fuzzy logic control in the AR detection.\n",
    "* `min_lat`: float, degree North, exclude systems whose centroids are lower than this latitude.\n",
    "* `max_lat`: float, degree North, exclude systems whose centroids are higher than this latitude.\n",
    "* `min_length`: float, km, AR candidates shorter than this length are flagged as `relaxed`.\n",
    "* `min_length_hard`: float, km, AR candidates shorter than this length are discarded.\n",
    "* `rdp_thres`: float, degree latitude/longitude, the user given error when simplifying axis using [rdp algorithm](https://en.wikipedia.org/wiki/Ramer%E2%80%93Douglas%E2%80%93Peucker_algorithm).\n",
    "* `fill_radius`: this is to fill up some holes found in the AR candidate regions. This can happen when your input data have pretty high resolution and tend to have more small scale features in the IVT field. Sometimes this will leave some small holes in the found AR region.\n",
    "* `max_ph_ratio`: float in (0,1). Maximum prominence/height ratio of a local peak. Only used when SINGLE_DOME=True.\n",
    "* `edge_eps`: float in (0,1). Minimal proportion of flux component in a direction to total flux to\n",
    "    allow edge building in that direction. Setting this to a higher value will impose greater restriction upon the directions of the AR axis, requiring it to more strictly follow the vectors of IVT. Setting a lower value gives more maneuver space of the AR axis to pass through the AR region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAM_DICT={\n",
    "    # kg/m/s, define AR candidates as regions >= than this anomalous ivt.\n",
    "    'thres_low' : 1,\n",
    "    # km^2, drop AR candidates smaller than this area.\n",
    "    'min_area': 50*1e4,\n",
    "    # km^2, drop AR candidates larger than this area.\n",
    "    'max_area': 1800*1e4,\n",
    "    # float, isoperimetric quotient. ARs larger than this (more circular in shape) is treated as relaxed.\n",
    "    'max_isoq': 0.6,\n",
    "    # float, isoperimetric quotient. ARs larger than this is discarded.\n",
    "    'max_isoq_hard': 0.7,\n",
    "    # degree, exclude systems whose centroids are lower than this latitude.\n",
    "    'min_lat': 20,\n",
    "    # degree, exclude systems whose centroids are higher than this latitude.\n",
    "    'max_lat': 80,\n",
    "    # km, ARs shorter than this length is treated as relaxed.\n",
    "    'min_length': 2000,\n",
    "    # km, ARs shorter than this length is discarded.\n",
    "    'min_length_hard': 1000,\n",
    "    # degree lat/lon, error when simplifying axis using rdp algorithm.\n",
    "    'rdp_thres': 2,\n",
    "    # grids. Remove small holes in AR contour.\n",
    "    'fill_radius': max(1,int(4*0.75/RESO)),\n",
    "    # max prominence/height ratio of a local peak. Only used when SINGLE_DOME=True\n",
    "    'max_ph_ratio': 0.4,\n",
    "    # minimal proportion of flux component in a direction to total flux to\n",
    "    # allow edge building in that direction\n",
    "    'edge_eps': 0.4\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------Import modules-------------------------\n",
    "import os\n",
    "import sys\n",
    "import cdms2 as cdms\n",
    "import MV2 as MV\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import measure\n",
    "from skimage import morphology\n",
    "\n",
    "from utils import funcs,plot\n",
    "from river_tracker1_funcs import areaFilt, maskToGraph, getARAxis, cropMask,\\\n",
    "        partPeaks, getARData, uvDecomp, save2DF, plotAR\n",
    "from river_tracker1 import findARs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then read in the input data, and do some slicing to select the part of data we want:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------Read in flux data----------------------\n",
    "qu=funcs.readVar(UQ_FILE_NAME, UQ_VAR)\n",
    "\n",
    "qv=funcs.readVar(VQ_FILE_NAME, VQ_VAR)\n",
    "\n",
    "#-----------------Shift longitude-----------------\n",
    "qu=qu(longitude=(SHIFT_LON,SHIFT_LON+360))\n",
    "qv=qv(longitude=(SHIFT_LON,SHIFT_LON+360))\n",
    "\n",
    "#-------------------Read in ivt-------------------\n",
    "print('\\n### <river_tracker1>: Read in file:\\n',IVT_FILE_NAME)\n",
    "fin=cdms.open(IVT_FILE_NAME,'r')\n",
    "ivt=fin('ivt')\n",
    "ivtrec=fin('ivt_rec')\n",
    "ivtano=fin('ivt_ano')\n",
    "fin.close()\n",
    "\n",
    "#--------------------Slice data--------------------\n",
    "qu=qu(time=(TIME_START,TIME_END), latitude=(LAT1, LAT2))(squeeze=1)\n",
    "qv=qv(time=(TIME_START,TIME_END), latitude=(LAT1, LAT2))(squeeze=1)\n",
    "ivt=ivt(time=(TIME_START,TIME_END))(squeeze=1)\n",
    "ivtrec=ivtrec(time=(TIME_START,TIME_END))(squeeze=1)\n",
    "ivtano=ivtano(time=(TIME_START,TIME_END))(squeeze=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then fetch the latitude/longitude axes from the data, and compute some geometrics using them.\n",
    "\n",
    "`timeax` stores the time axis information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------Get coordinates-----------------\n",
    "latax=qu.getLatitude()\n",
    "lonax=qu.getLongitude()\n",
    "\n",
    "dxs=funcs.dLongitude(qu,R=6371)\n",
    "dys=funcs.dLatitude(qu,R=6371)\n",
    "areamap=dxs*dys # km^2\n",
    "costhetas=dxs/MV.sqrt(dxs**2+dys**2)\n",
    "sinthetas=dys/MV.sqrt(dxs**2+dys**2)\n",
    "\n",
    "timeax=ivt.getTime().asComponentTime()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, make sure the output folders exist, and create an empty netcdf file to write the results. \n",
    "`result_dict` is an empty dict to save the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dict={}\n",
    "\n",
    "if not os.path.exists(OUTPUTDIR):\n",
    "    os.makedirs(OUTPUTDIR)\n",
    "\n",
    "if PLOT:\n",
    "    plot_dir=os.path.join(OUTPUTDIR, 'plots')\n",
    "    if not os.path.exists(plot_dir):\n",
    "        os.makedirs(plot_dir)\n",
    "\n",
    "file_out_name='ar_s_6_%d_label-angle-flux.nc' %YEAR\n",
    "abpath_out=os.path.join(OUTPUTDIR,file_out_name)\n",
    "print('\\n### <river_tracker1>: Saving output to:\\n',abpath_out)\n",
    "ncfout=cdms.open(abpath_out,'w')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All prepared, we can now start looping through the time steps and detecting ARs. This is done in a `for` loop over the `timeax`.\n",
    "\n",
    "The `findARs()` function does most of the heavy lifting work, including:\n",
    "\n",
    "* detect candidate regions satisfying requirements given in the `PARAM_DICT` dict.\n",
    "* for each passing candidate, compute an AR axis.\n",
    "\n",
    "The `getARData()` function fetches some information from each AR candidate, including:\n",
    "\n",
    "* its numerical label,\n",
    "* length,\n",
    "* area,\n",
    "* width (defined as area/length),\n",
    "* centroid coordinates,\n",
    "* axis coordinates,\n",
    "* contour coordinates,\n",
    "* etc.\n",
    "\n",
    "These info is saved as a `pandas.DataFrame` (`ardf`), and saved to the `result_dict` dict, using the time stamp as key.\n",
    "\n",
    "* `labels` is a netcdf variable saving the numerical labels of all found ARs in each time step. It has shape of `(time, lat, lon)`.\n",
    "* `angles` is a netcdf variable saving the difference in the orientation of IVT vectors in all found ARs, wrt the AR axis. It is not relevant at this stage.\n",
    "* `crossfluxes` is a netcdf variable saving the cross-sectional IVT flux, computed as the projection of IVT vectors onto the AR axis, using angles in `angles`. It is not relevant at this stage.\n",
    "\n",
    "Lastly, the fields of IVT, IVT reconstruction (`ivt_rec`) and IVT anomalies (`ivt_ano`) at each time point are plotted, superimposed with all found ARs. The outline of a `relaxed` AR is drawn with dashed lines, and a strict AR with solid lines. Recall that `relaxed` means they are \"kind of\" ARs but maybe not strictly so, most of such cases are systems that still in genesis stage or about to dissipate. The axis of an AR is also drawn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------Loop through time----------------\n",
    "for ii, timett in enumerate(timeax):\n",
    "\n",
    "    timett_str='%d-%02d-%02d %02d:00' %(timett.year,timett.month,\\\n",
    "            timett.day,timett.hour)\n",
    "\n",
    "    print('\\n# <river_tracker1>: Processing time: %s' %timett_str)\n",
    "\n",
    "    slab=ivt[ii]\n",
    "    slabano=ivtano[ii]\n",
    "    slabrec=ivtrec[ii]\n",
    "    quslab=qu[ii]\n",
    "    qvslab=qv[ii]\n",
    "\n",
    "    # decompose background-transient\n",
    "    qurec,quano,qvrec,qvano=uvDecomp(quslab,qvslab,slabrec,slabano)\n",
    "\n",
    "    # find ARs\n",
    "    mask_list,axis_list,armask,axismask=findARs(slabano,quano,qvano,\n",
    "            areamap,costhetas,sinthetas,PARAM_DICT)\n",
    "\n",
    "    # skip if none\n",
    "    if armask.sum()==0:\n",
    "        continue\n",
    "\n",
    "    # fetch AR related data\n",
    "    labels,angles,crossfluxes,ardf=getARData(\n",
    "            slab,quslab,qvslab,\n",
    "            slabano,quano,qvano,\n",
    "            areamap,\n",
    "            mask_list,axis_list,timett_str,PARAM_DICT,SHIFT_LON,\n",
    "            False,OUTPUTDIR)\n",
    "\n",
    "    # prepare nc output\n",
    "    timeaxii=cdms.createAxis([timett.torel('days since 1900-1-1').value])\n",
    "    timeaxii.designateTime()\n",
    "    timeaxii.id='time'\n",
    "    timeaxii.units='days since 1900-1-1'\n",
    "\n",
    "    labels=funcs.addExtraAxis(labels,timeaxii)\n",
    "    angles=funcs.addExtraAxis(angles,timeaxii)\n",
    "    crossfluxes=funcs.addExtraAxis(crossfluxes,timeaxii)\n",
    "\n",
    "    # save to disk\n",
    "    ncfout.write(labels,typecode='f')\n",
    "    ncfout.write(angles,typecode='f')\n",
    "    ncfout.write(crossfluxes,typecode='f')\n",
    "\n",
    "    result_dict[timett_str]=ardf\n",
    "\n",
    "    #-------------------Plot------------------------\n",
    "    if PLOT:\n",
    "        plot_vars=[slab,slabrec,slabano]\n",
    "        titles=['IVT', 'Reconstruction', 'THR']\n",
    "        iso=plot.Isofill(plot_vars,12,1,1,min_level=0,max_level=800)\n",
    "\n",
    "        figure=plt.figure(figsize=(12,10),dpi=100)\n",
    "\n",
    "        for jj in range(len(plot_vars)):\n",
    "            ax=figure.add_subplot(3,1,jj+1)\n",
    "            pobj=plot.plot2(plot_vars[jj],iso,ax,projection='cyl',\n",
    "                    title='%s %s' %(timett_str, titles[jj]),\n",
    "                    fix_aspect=False)\n",
    "\n",
    "            bmap=pobj.bmap\n",
    "            plotAR(ardf,ax,bmap)\n",
    "\n",
    "        #----------------- Save plot------------\n",
    "        plot_save_name='ar_%s' %(timett_str)\n",
    "        plot_save_name=os.path.join(plot_dir,plot_save_name)\n",
    "        print('\\n# <river_tracker1>: Save figure to', plot_save_name)\n",
    "        figure.savefig(plot_save_name+'.png',dpi=100,bbox_inches='tight')\n",
    "\n",
    "        plt.close('all')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After finishing the time loop, we do some post-processing of the data saved in the `result_dict` dict, make into a big `pandas.DataFrame` object, and save it to a `csv` file.\n",
    "\n",
    "Calling `ncfout.close()` makes sure the netcdf file is properly closed.\n",
    "\n",
    "The `np.set_printoptions()` function makes sure that in the saved `csv` table, the value in any cell does not contain any ellipsis `...`. This is because the coordinates of an AR axis is a list of float numbers. When this list goes too long, an ellipsis will be inserted in the saved `csv` output, e.g.\n",
    "\n",
    "```\n",
    "[12.232, 15.234, 17.3435, ..., 20.123, 24.333]\n",
    "```\n",
    "\n",
    "The same is also true for the AR contour coordinates.\n",
    "\n",
    "To prevent this, the `np.set_printoptions()` function is called, with different input arguments for py2 and py3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncfout.close()\n",
    "\n",
    "result_df=save2DF(result_dict)\n",
    "file_out_name='ar_records_%s-%s.csv'\\\n",
    "        %(TIME_START.replace(' ','_').replace(':','-'),\n",
    "        TIME_END.replace(' ','_').replace(':','-'))\n",
    "\n",
    "abpath_out=os.path.join(OUTPUTDIR,file_out_name)\n",
    "print('\\n### <river_tracker1>: Saving output to:\\n',abpath_out)\n",
    "# Necessary: to remove ... in csv file\n",
    "if sys.version_info.major==2:\n",
    "    np.set_printoptions(threshold='nan')\n",
    "elif sys.version_info.major==3:\n",
    "    np.set_printoptions(threshold=9223372036854775807)\n",
    "result_df.to_csv(abpath_out,index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show some results\n",
    "\n",
    "After the computation is done, we could have a look at some of the results.\n",
    "\n",
    "First print the number of ARs detected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of ARs found during %s - %s = %d.\" %(TIME_START, TIME_END, len(result_df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then print the first few records:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns of this table:\n",
    "\n",
    "* **id**: integer numeric id for this AR at *this particular* time point. ARs at different time points can share the same **id**, and an AR can be uniquely identified with the combination of time stamp + id.\n",
    "* **time**: time stamp in the YYYY-MM-DD HH:mm:ss format.\n",
    "* **contour_y**: list of floats, the y-coordinates (latitudes) of the AR contour in degrees North.\n",
    "* **contour_x**: list of floats, the x-coordinates (longitude) of the AR contour in degrees North.\n",
    "* **centroid_y**: float, latitude of the AR centroid, weighted by the IVT value.\n",
    "* **centroid_x**: float, longitude of the AR centroid, weighted by the IVT value.\n",
    "* **axis_y**: list of floats, latitudes of the AR axis.\n",
    "* **axis_x**: list of floats, longitude of the AR axis.\n",
    "* **axis_rdp_y**: list of floats, latitude of the simplified AR axis.\n",
    "* **axis_rdp_x**: list of floats, longitude of the simplified AR axis.\n",
    "* **area**: float, area of the AR in $km^2$.\n",
    "* **length**: float, length of the AR in $km$.\n",
    "* **width**: float, effective width in $km$, as area/length.\n",
    "* **iso_quotient**: float, isoperimeteric quotient.\n",
    "* **LW_ratio**: float, length/width ratio.\n",
    "* **strength**: float, spatially averaged IVT value within the AR region, in $kg/m/s$.\n",
    "* **strength_ano**: float, spatially averaged *anomalous* IVT value within the AR region, in $kg/m/s$.\n",
    "* **strength_std**: float, standard deviation of IVT within the AR region, in $kg/m/s$.\n",
    "* **max_strength**: float, maximum IVT value within the AR region, in $kg/m/s$.\n",
    "* **mean_angle**: float, spatially averaged angle between the IVT vector and the AR axis, in degrees.\n",
    "* **is_relaxed**: True or False, whether the AR is flagged as \"relaxed\".\n",
    "* **qv_mean**: float, spatially averaged meridional integrated vapor flux, in $kg/m/s$.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create some plots at the last time step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_vars=[slab,slabrec,slabano]\n",
    "titles=['IVT', 'Reconstruction', 'THR']\n",
    "iso=plot.Isofill(plot_vars,12,1,1,min_level=0,max_level=800)\n",
    "\n",
    "figure=plt.figure(figsize=(12,10),dpi=100)\n",
    "\n",
    "for jj in range(len(plot_vars)):\n",
    "    ax=figure.add_subplot(3,1,jj+1)\n",
    "    pobj=plot.plot2(plot_vars[jj],iso,ax,projection='cyl',\n",
    "             title='%s %s' %(timett_str, titles[jj]),\n",
    "            fix_aspect=False)\n",
    "\n",
    "    bmap=pobj.bmap\n",
    "    plotAR(ardf,ax,bmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
