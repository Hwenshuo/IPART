{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detect ARs from individual time steps\n",
    "\n",
    "\n",
    "This notebook detects ARs at instantaneous time steps.\n",
    "\n",
    "The detection is performed mainly on the anomalous IVT computed in the previous step (in notebook `2 compute_THR`), using these steps:\n",
    "\n",
    "1. At any time point, find all contiguous regions from the `ivt_ano` field where `ivt_ano` $> 0\\, kg/(m \\cdot s)$.\n",
    "2. Compute the centroid of all such regions, using the underlying IVT values as weights.\n",
    "3. Discard all regions whose area is $\\le 50 \\times 10^4\\, km^2$, or $\\ge 1800 \\times 10^4\\,km^2$.\n",
    "4. Discard all regions whose centroid lies north of $80^{\\circ}\\, N$, or south of $20^{\\circ} N$.\n",
    "6. Compute the AR axis.\n",
    "7. Discard all regions whose AR axis is $\\le 1500\\, km$.\n",
    "8. Compute the effective width as area/length, and the length/width ratio.\n",
    "9. Discard all regions whose length/width ratio is $\\le 2$, if length $< 2000\\, km$.\n",
    "\n",
    "All passing systems after the above steps are regarded as ARs.\n",
    "\n",
    "There are some more details given in the parameter selection section down below.\n",
    "\n",
    "In production, you can use the `scripts/detect_ARs.py` or `scripts/detect_ARs_generator_version.py` for this step.\n",
    "\n",
    "\n",
    "## Input data\n",
    "\n",
    "* `uflux_s_6_1984_Jan.nc`: u-component of vertically integrated vapor fluxes (`standard_name`: `eastward_atmosphere_water_transport_across_unit_distance`), in $kg/(m \\cdot s)$.\n",
    "* `vflux_s_6_1984_Jan.nc`: v-component of vertically integrated vapor fluxes (`standard_name`: `northward_atmosphere_water_transport_across_unit_distance`, in $kg/(m \\cdot s)$.\n",
    "* `ivt_s_6_1984_Jan-THR-kernel-t16-s6.nc`: decomposition of IVT using the THR algorithm, into a reconstruction component (`ivt_rec`) and the anomaly component (`ivt_ano`), in $kg/(m \\cdot s)$.\n",
    "\n",
    "\n",
    "## Steps\n",
    "\n",
    "1. Make sure you have successfully run the previous notebook.\n",
    "2. Execute the following code blocks in sequence.\n",
    "\n",
    "\n",
    "## Results\n",
    "\n",
    "\n",
    "* `ar_records.csv`: a csv table listing various attributes for each detected AR appearance.\n",
    "* `ar_s_6_1984_labels.nc`: a netCDF data file saving these 3 variables:\n",
    "    * `label`: integer labels distinguishing all ARs detected at individual time steps.\n",
    "    * `angles`: the angle between horizontal vapor flux and the local AR axis, in degrees.\n",
    "    * `ivt_cross`: cross-sectional IVT flux, computed as the product of IVT vector and cosine of `angles`.\n",
    "* `plots/ar_YYYY-MM-DD HH:00.png` (optional): plots of IVT and detected ARs at individual time steps.\n",
    "\n",
    "All results will be saved to the same folder as this notebook file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set some global parameters\n",
    "\n",
    "`YEAR`, `TIME_START`, and `TIME_END` are used to specify the time domain to process.\n",
    "\n",
    "In this notebook only a small time subset is specified. In production you would want to set the time points to suit your needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------Time range--------------------\n",
    "YEAR=1984\n",
    "TIME_START='%d-01-10 00:00:00' %YEAR\n",
    "TIME_END='%d-01-20 00:00:00' %YEAR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify the input and output locations. A subfolder is created using the `YEAR` defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os, sys\n",
    "\n",
    "#-----------u-qflux----------------------\n",
    "UQ_FILE_NAME=os.path.join('.', 'uflux_s_6_1984_Jan.nc')\n",
    "UQ_VAR='p71.162'\n",
    "\n",
    "#-----------v-qflux----------------------\n",
    "VQ_FILE_NAME=os.path.join('.', 'vflux_s_6_1984_Jan.nc')\n",
    "VQ_VAR='p72.162'\n",
    "\n",
    "#-----------------ivt reconstruction and anomalies-----------------\n",
    "IVT_FILE_NAME=os.path.join('.', 'ivt_s_6_1984_Jan-THR-kernel-t16-s6.nc')\n",
    "\n",
    "#------------------Output folder------------------\n",
    "OUTPUTDIR=os.path.join('.', str(YEAR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `PLOT` controls whether to plot figures of IVT with detected ARs.\n",
    "* `SHIFT_LON` shifts the data along the x-dimension by 80 degrees so the Pacific and Atlantic oceans are centered.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PLOT=True          # create maps of found ARs or not\n",
    "SHIFT_LON=80          # degree, shift left bound to longitude. Should match\n",
    "                      # that used in compute_thr_singlefile.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `PARAM_DICT` dictionary contains important parameters used in the detection process:\n",
    "\n",
    "* `thres_low`: float, a minimum IVT value in $kg/(m \\cdot s)$. This tells the script to look for AR candidates as regions where the anomaly IVT (`ivt_ano`) >= this value. This is the same idea as the IVT250 thresholding method. In production one should give it a small nominal value like `1`, `10` etc., or just `0`.\n",
    "* `min_area`: float, minimum area in $km^2$. Drop AR candidates smaller than this area. Region of area is defined as the summation of grid cell areas, computed using the latitude/longitude meta data of input data. This is used to filter out some miniature features.\n",
    "* `max_area`: float, maximum area in $km^2$. Filter out regions too large in size. This might happen when 2 AR-like features get merged together. You can prevent this from happening by raising the `thres_low` value, or setting `SINGLE_DOME` to true.\n",
    "* `min_LW`: float, minimal length/width ratio.\n",
    "* `min_lat`: float, degree North, exclude systems whose centroids are lower than this latitude.\n",
    "* `max_lat`: float, degree North, exclude systems whose centroids are higher than this latitude.\n",
    "* `min_length`: float, km, AR candidates shorter than this length are flagged as `relaxed`.\n",
    "* `min_length_hard`: float, km, AR candidates shorter than this length are discarded.\n",
    "* `rdp_thres`: float, degree latitude/longitude, the user given error when simplifying axis using [rdp algorithm](https://en.wikipedia.org/wiki/Ramer%E2%80%93Douglas%E2%80%93Peucker_algorithm).\n",
    "* `fill_radius`: this is to fill up some holes found in the AR candidate regions. This can happen when your input data have pretty high resolution and tend to have more small scale features in the IVT field. Sometimes this will leave some small holes in the found AR region. Set to `None` will compute a default value based on data resolution.\n",
    "* `single_dome`: `True` or `False` values control whether to separate local IVT maxima that are merged into one contour.\n",
    "* `max_ph_ratio`: float in (0,1). Maximum prominence/height ratio of a local peak. Only used when SINGLE_DOME=True.\n",
    "* `edge_eps`: float in (0,1). Minimal proportion of flux component in a direction to total flux to\n",
    "    allow edge building in that direction. Setting this to a higher value will impose greater restriction upon the directions of the AR axis, requiring it to more strictly follow the vectors of IVT. Setting a lower value gives more maneuver space of the AR axis to pass through the AR region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAM_DICT={\n",
    "    # kg/m/s, define AR candidates as regions >= than this anomalous ivt.\n",
    "    'thres_low' : 1,\n",
    "    # km^2, drop AR candidates smaller than this area.\n",
    "    'min_area': 50*1e4,\n",
    "    # km^2, drop AR candidates larger than this area.\n",
    "    'max_area': 1800*1e4,\n",
    "    # float, minimal length/width ratio.\n",
    "    'min_LW': 2,\n",
    "    # degree, exclude systems whose centroids are lower than this latitude.\n",
    "    'min_lat': 20,\n",
    "    # degree, exclude systems whose centroids are higher than this latitude.\n",
    "    'max_lat': 80,\n",
    "    # km, ARs shorter than this length is treated as relaxed.\n",
    "    'min_length': 2000,\n",
    "    # km, ARs shorter than this length is discarded.\n",
    "    'min_length_hard': 1500,\n",
    "    # degree lat/lon, error when simplifying axis using rdp algorithm.\n",
    "    'rdp_thres': 2,\n",
    "    # grids. Remove small holes in AR contour.\n",
    "    'fill_radius': None,\n",
    "    # do peak partition or not, used to separate systems that are merged\n",
    "    # together with an outer contour.\n",
    "    'single_dome': False,\n",
    "    # max prominence/height ratio of a local peak. Only used when single_dome=True\n",
    "    'max_ph_ratio': 0.6,\n",
    "    # minimal proportion of flux component in a direction to total flux to\n",
    "    # allow edge building in that direction\n",
    "    'edge_eps': 0.4\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------Import modules-------------------------\n",
    "import os, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "from ipart.utils import funcs,plot\n",
    "from ipart.AR_detector import plotAR, findARs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then read in the input data, and do some slicing to select the part of data we want:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------Read in flux data----------------------\n",
    "qu=funcs.readNC(UQ_FILE_NAME, UQ_VAR)\n",
    "qv=funcs.readNC(VQ_FILE_NAME, VQ_VAR)\n",
    "\n",
    "#-------------------Read in ivt-------------------\n",
    "print('\\n# Read in file:\\n',IVT_FILE_NAME)\n",
    "ivt=funcs.readNC(IVT_FILE_NAME, 'ivt')\n",
    "ivtrec=funcs.readNC(IVT_FILE_NAME, 'ivt_rec')\n",
    "ivtano=funcs.readNC(IVT_FILE_NAME, 'ivt_ano')\n",
    "\n",
    "#-----------------Shift longitude-----------------\n",
    "qu=qu.shiftLon(SHIFT_LON)\n",
    "qv=qv.shiftLon(SHIFT_LON)\n",
    "ivt=ivt.shiftLon(SHIFT_LON)\n",
    "ivtrec=ivtrec.shiftLon(SHIFT_LON)\n",
    "ivtano=ivtano.shiftLon(SHIFT_LON)\n",
    "\n",
    "#--------------------Slice data--------------------\n",
    "qu=qu.sliceData(TIME_START,TIME_END,axis=0).squeeze()\n",
    "qv=qv.sliceData(TIME_START,TIME_END,axis=0).squeeze()\n",
    "ivt=ivt.sliceData(TIME_START,TIME_END,axis=0).squeeze()\n",
    "ivtrec=ivtrec.sliceData(TIME_START,TIME_END,axis=0).squeeze()\n",
    "ivtano=ivtano.sliceData(TIME_START,TIME_END,axis=0).squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then fetch the time, latitude/longitude axes from the data. The time axis is a list of `datetime.datetime` objects, e.g. `['1984-01-01 00:00', '1984-01-01 06:00', ..., '1984-05-31 00:00']`. The latitude/longitude axis is assumed to be a 1d array storing the latitude/longitude coordinates in degrees of lat/lon.\n",
    "\n",
    "In this script we are using the `netcdf4` to fetch these metadata from the netCDF variables. If you are using other packages like `CDAT`, `xarray` or `iris`, please adjust the relevant codes accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------Get coordinates-----------------\n",
    "latax=qu.getLatitude()\n",
    "lonax=qu.getLongitude()\n",
    "timeax=ivt.getTime()\n",
    "timeax=['%d-%02d-%02d %02d:00' %(timett.year, timett.month, timett.day, timett.hour) for timett in timeax]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All prepared, we can now call the detection function `findARs()` that does most of the heavy lifting work, including:\n",
    "\n",
    "* detect candidate regions satisfying requirements given in the `PARAM_DICT` dict.\n",
    "* for each passing candidate, compute an AR axis.\n",
    "* fetches some information from each AR, including:\n",
    "    * its numerical label,\n",
    "    * length,\n",
    "    * area,\n",
    "    * width (defined as area/length),\n",
    "    * centroid coordinates,\n",
    "    * axis coordinates,\n",
    "    * contour coordinates,\n",
    "    * average IVT strength,\n",
    "    * others.\n",
    "\n",
    "These information is saved in a `pandas.DataFrame` named `result_df`.\n",
    "\n",
    "`findARs()` also outputs these variables:\n",
    "\n",
    "* `time_idx`: a list of indices of the time dimension when any AR is found.\n",
    "* `labels` is a netcdf variable saving the numerical labels of all found ARs in each time step. It has shape of `(time, lat, lon)`.\n",
    "* `angles` is a netcdf variable saving the difference in the orientation of IVT vectors in all found ARs, wrt the AR axis. It is not relevant at this stage.\n",
    "* `crossfluxes` is a netcdf variable saving the cross-sectional IVT flux, computed as the projection of IVT vectors onto the AR axis, using angles in `angles`. It is not relevant at this stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "time_idx, labels, angles, crossfluxes, result_df = findARs(ivt.data, ivtrec.data,\n",
    "            ivtano.data, qu.data, qv.data, latax, lonax, times=timeax, **PARAM_DICT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show some results\n",
    "\n",
    "After the computation is done, we could have a look at some of the results.\n",
    "\n",
    "First print the number of ARs detected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of ARs found during %s - %s = %d.\" %(TIME_START, TIME_END, len(result_df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then print the first few records:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result_df.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns of this table:\n",
    "\n",
    "* **id**: integer numeric id for this AR at *this particular* time point. ARs at different time points can share the same **id**, and an AR can be uniquely identified with the combination of time stamp + id.\n",
    "* **time**: time stamp in the YYYY-MM-DD HH:mm:ss format.\n",
    "* **contour_y**: list of floats, the y-coordinates (latitudes) of the AR contour in degrees North.\n",
    "* **contour_x**: list of floats, the x-coordinates (longitude) of the AR contour in degrees North.\n",
    "* **centroid_y**: float, latitude of the AR centroid, weighted by the IVT value.\n",
    "* **centroid_x**: float, longitude of the AR centroid, weighted by the IVT value.\n",
    "* **axis_y**: list of floats, latitudes of the AR axis.\n",
    "* **axis_x**: list of floats, longitude of the AR axis.\n",
    "* **axis_rdp_y**: list of floats, latitude of the simplified AR axis.\n",
    "* **axis_rdp_x**: list of floats, longitude of the simplified AR axis.\n",
    "* **area**: float, area of the AR in $km^2$.\n",
    "* **length**: float, length of the AR in $km$.\n",
    "* **width**: float, effective width in $km$, as area/length.\n",
    "* **LW_ratio**: float, length/width ratio.\n",
    "* **strength**: float, spatially averaged IVT value within the AR region, in $kg/(m \\cdot s)$.\n",
    "* **strength_ano**: float, spatially averaged *anomalous* IVT value within the AR region, in $kg/ (m \\cdot s)$.\n",
    "* **strength_std**: float, standard deviation of IVT within the AR region, in $kg/(m \\cdot s)$.\n",
    "* **max_strength**: float, maximum IVT value within the AR region, in $kg/(m \\cdot s)$.\n",
    "* **mean_angle**: float, spatially averaged angle between the IVT vector and the AR axis, in degrees.\n",
    "* **is_relaxed**: True or False, whether the AR is flagged as \"relaxed\".\n",
    "* **qv_mean**: float, spatially averaged meridional integrated vapor flux, in $kg/(m \\cdot s)$.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create some plots for the 1st time step with any detected ARs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_idx=time_idx[0]\n",
    "\n",
    "plot_time=timeax[plot_idx]\n",
    "slab=ivt.data[plot_idx]\n",
    "slabrec=ivtrec.data[plot_idx]\n",
    "slabano=ivtano.data[plot_idx]\n",
    "ardf=result_df[result_df.time==plot_time]\n",
    "\n",
    "plot_vars=[slab, slabrec, slabano]\n",
    "titles=['IVT', 'THR_recon', 'THR_ano']\n",
    "iso=plot.Isofill(plot_vars, 12, 1, 1,min_level=0,max_level=800)\n",
    "\n",
    "figure=plt.figure(figsize=(12,10), dpi=100)\n",
    "\n",
    "for jj in range(len(plot_vars)):\n",
    "    ax=figure.add_subplot(3,1,jj+1,projection=ccrs.PlateCarree())\n",
    "    pobj=plot.plot2(plot_vars[jj], iso, ax,\n",
    "            xarray=lonax, yarray=latax,                    \n",
    "            title='%s %s' %(plot_time, titles[jj]),\n",
    "            fix_aspect=False)\n",
    "    plotAR(ardf, ax, lonax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, save the outputs to disk.\n",
    "\n",
    "The `result_df` is saved to a `.csv` file.\n",
    "\n",
    "The `np.set_printoptions()` function makes sure that in the saved `csv` table, the value in any cell does not contain any ellipsis `...`. This is because the coordinates of an AR axis is a list of float numbers. When this list goes too long, an ellipsis will be inserted in the saved `csv` output, e.g.\n",
    "\n",
    "```\n",
    "[12.232, 15.234, 17.3435, ..., 20.123, 24.333]\n",
    "```\n",
    "\n",
    "The same is also true for the AR contour coordinates.\n",
    "\n",
    "To prevent this, the `np.set_printoptions()` function is called, with different input arguments for py2 and py3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(OUTPUTDIR):\n",
    "    os.makedirs(OUTPUTDIR)\n",
    "\n",
    "abpath_out=os.path.join(OUTPUTDIR, 'ar_records.csv')\n",
    "print('\\n# Saving output to:\\n',abpath_out)\n",
    "# Necessary: to remove ... in csv file\n",
    "if sys.version_info.major==2:\n",
    "    np.set_printoptions(threshold=np.inf)\n",
    "elif sys.version_info.major==3:\n",
    "    np.set_printoptions(threshold=sys.maxsize)\n",
    "result_df.to_csv(abpath_out,index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then save the labels to a `.nc` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abpath_out=os.path.join(OUTPUTDIR, 'ar_s_6_%d_labels.nc' %YEAR)\n",
    "print('\\n# Saving output to:\\n',abpath_out)\n",
    "funcs.saveNC(abpath_out, labels, 'w')\n",
    "funcs.saveNC(abpath_out, angles, 'a')\n",
    "funcs.saveNC(abpath_out, crossfluxes, 'a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
